{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3655f03-1494-4678-9885-3e859882b97e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45d56b55-2a11-491f-b173-4deaf2a31472",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor, Resize, Compose\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import dataset\n",
    "from torchvision.ops import box_iou\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f9c8d69-fdcb-4cf5-8db6-954801fb9d05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pascal_voc_train = torchvision.datasets.VOCDetection(\n",
    "    root=\"../data\",\n",
    "    year=\"2007\",\n",
    "    image_set=\"train\",\n",
    "    download=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d15f266e-7ac2-4b57-8e3f-146886742030",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRANSFORMING PASCAL VOC\n"
     ]
    }
   ],
   "source": [
    "voc_train = dataset.PascalVOC(pascal_voc=pascal_voc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70f22340-8e61-4875-8f02-d13102365e83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def iou():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "864f6c31-62c8-40fc-b00f-4767443189d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class YOLOv1Loss(nn.Module):\n",
    "    \"\"\"\n",
    "    YOLOv1 Loss\n",
    "    \"\"\"\n",
    "    def __init__(self, S=7, B=2, C=20, lambda_coord=5, lambda_noobj=0.5):\n",
    "        \"\"\"\n",
    "        S: dimension of the S x S grid\n",
    "        B: number of bounding boxes predicted by network\n",
    "        C: number of classes\n",
    "        lambda_coord: penalty for coord loss\n",
    "        lambda_noobj: penalty for confidence loss when no object is present in target\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "        self.lambda_coord = lambda_coord\n",
    "        self.lambda_noobj = lambda_noobj\n",
    "        \n",
    "    def xywh_to_x1y1x2y2(self, boxes: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Converts YOLO bounding box format to (x1, y1, x2, y2)\n",
    "        \n",
    "        pred: (X, 4)\n",
    "        \n",
    "        returns (X, 4)\n",
    "        \"\"\"\n",
    "        x = boxes[..., 0] # (N, S^2, X)\n",
    "        y = boxes[..., 1]\n",
    "        w = boxes[..., 2]\n",
    "        h = boxes[..., 3]\n",
    "        \n",
    "        x1 = x - w / 2\n",
    "        y1 = y - h / 2\n",
    "        x2 = x + w / 2\n",
    "        y2 = y + h / 2\n",
    "        \n",
    "        x1y1x2y2 = torch.stack((x1, y1, x2, y2), dim=1)\n",
    "        \n",
    "        return x1y1x2y2\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        \"\"\"\n",
    "        pred: (N x S x S x (5 * B + C))\n",
    "        target: (N x S x S x (5 + C))\n",
    "        \"\"\"    \n",
    "        print(\"CALCULATING YOLO LOSS\")\n",
    "        \n",
    "        # check pred and target are in the correct shape\n",
    "        assert len(pred) == len(target)\n",
    "        N = pred.size(0)\n",
    "        \n",
    "        print(f\"BATCH SIZE: {N}\")\n",
    "        \n",
    "        # get parameters of YOLO loss\n",
    "        S = self.S\n",
    "        B = self.B\n",
    "        C = self.C\n",
    "        lambda_coord = self.lambda_coord\n",
    "        lambda_noobj = self.lambda_noobj\n",
    "        \n",
    "        assert pred.shape == torch.Size((N, S, S, 5 * B + C))\n",
    "        assert target.shape == torch.Size((N, S, S, 5 + C))\n",
    "        \n",
    "        # set requires_grad=True\n",
    "        # pred = pred.clone().requires_grad_()\n",
    "        # target = target.clone().requires_grad_()\n",
    "        \n",
    "        # obj, noobj mask: select bounding boxes whose target bounding box has a confidence=1 for obj and confidence=0\n",
    "        # for noobj\n",
    "        obj_mask = target[:, :, :, 0] == 1\n",
    "        noobj_mask = target[:, :, :, 0] == 0\n",
    "        \n",
    "        # select predictions and targets where ground truth contains an object\n",
    "        obj_pred = pred[obj_mask] # (num_obj, 5*B+C)\n",
    "        obj_target = target[obj_mask] # (num_obj, 5+C)\n",
    "        \n",
    "        # get bounding boxes\n",
    "        obj_pred_bndbox = obj_pred[:, :5*B].view(-1, B, 5) # (num_obj, 5*B+C) -> (num_obj, B, 5)\n",
    "        obj_target_bndbox = obj_target[:, :5].view(-1, 1, 5) # (num_obj, 5*B+C) -> (num_obj, 1, 5)\n",
    "        \n",
    "        print(\"BOUNDING BOXES\")\n",
    "        print(obj_pred_bndbox, obj_pred_bndbox.shape)\n",
    "        print(obj_target_bndbox, obj_target_bndbox.shape)\n",
    "        \n",
    "        print(\"OBJ PRED\")\n",
    "        print(obj_pred, obj_pred.shape)\n",
    "        \n",
    "        print(\"OBJ TARGET\")\n",
    "        print(obj_target, obj_target.shape)\n",
    "        \n",
    "        # select predictions and targets where ground grouth does not contain an object\n",
    "        noobj_pred = pred[noobj_mask] # (num_noobj, 5*B+C)\n",
    "        noobj_target = target[noobj_mask] # (num_obj, 5+C)\n",
    "        \n",
    "        # get bounding boxes for target's whose confidenc=0\n",
    "        noobj_pred_bndbox = noobj_pred[:, :5*B].view(-1, B, 5) # (num_noobj, 5*B+C) -> (num_noobj, B, 5)\n",
    "        noobj_target_bndbox = noobj_target[:, :5].view(-1, 1, 5)  # (num_noobj, 5*B+C) -> (num_noobj, 1, 5)\n",
    "        \n",
    "        # calculate ious\n",
    "        max_iou_mask = torch.BoolTensor(obj_pred_bndbox.size())\n",
    "        \n",
    "        print(\"SELECTING PREDICTOR BOXES\")\n",
    "        for i in range(obj_pred_bndbox.size(0)):\n",
    "            # get proposed boxes and target box\n",
    "            pred_bndbox = obj_pred_bndbox[i][:, 1:] # (B, 4)\n",
    "            target_bndbox = obj_target_bndbox[i][:, 1:] # (1, 4)\n",
    "            \n",
    "            # convert (x, y, w, h) -> (x1, y1, x2, y2)\n",
    "            pred_bndbox = self.xywh_to_x1y1x2y2(pred_bndbox)\n",
    "            target_bndbox = self.xywh_to_x1y1x2y2(target_bndbox)\n",
    "        \n",
    "            # get box ious\n",
    "            ious = box_iou(pred_bndbox, target_bndbox).squeeze(-1) # (B)\n",
    "            \n",
    "            # get the box with the max iou and keep in mask\n",
    "            max_iou, max_idx = ious.max(dim=0)\n",
    "            max_iou_mask[i, max_idx] = 1\n",
    "\n",
    "        # responsible predictors\n",
    "        obj_pred_bndbox = obj_pred_bndbox[max_iou_mask].view(-1, 5) # (num_obj, 5)    \n",
    "        obj_target_bndbox = obj_target_bndbox.squeeze(1) # (num_obj, 5)\n",
    "        \n",
    "        print(\"RESPONSIBLE PREDICTORS\")\n",
    "        print(obj_pred_bndbox, obj_pred_bndbox.shape)\n",
    "        print(obj_target_bndbox, obj_target_bndbox.shape)\n",
    "     \n",
    "        ###\n",
    "        # Bounding Box Loss\n",
    "        ###\n",
    "        print(\"BOUNDING BOX Loss\")\n",
    "        pred_xy = obj_pred_bndbox[:, 1:3]\n",
    "        target_xy = obj_target_bndbox[:, 1:3]\n",
    "        xy_loss = lambda_coord * F.mse_loss(pred_xy, target_xy, reduction=\"sum\")\n",
    "        print(xy_loss)\n",
    "          \n",
    "        pred_wh = torch.sqrt(obj_pred_bndbox[:, 3:5])\n",
    "        target_wh = torch.sqrt(obj_target_bndbox[:, 3:5])\n",
    "        wh_loss = lambda_coord * F.mse_loss(pred_wh, target_wh, reduction=\"sum\")\n",
    "        print(wh_loss)\n",
    "        \n",
    "        localization_loss = xy_loss + wh_loss\n",
    "        \n",
    "        ###\n",
    "        # Confidence Loss\n",
    "        ###\n",
    "        print(\"CONFIDENCE Loss\")\n",
    "        obj_pred_confidence = obj_pred_bndbox[:, 0]\n",
    "        obj_target_confidence = obj_target_bndbox.squeeze(1)[:, 0]\n",
    "        obj_confidence_loss = F.mse_loss(obj_pred_confidence, obj_target_confidence, reduction=\"sum\")        \n",
    "        \n",
    "        print(noobj_pred_bndbox, noobj_pred_bndbox.shape)\n",
    "        print(noobj_target_bndbox, noobj_target_bndbox.shape)\n",
    "        print(obj_confidence_loss)\n",
    "        \n",
    "        noobj_pred_confidence = noobj_pred_bndbox[:, :, 0] # (num_noobj, 2)\n",
    "        noobj_target_confidence = noobj_target_bndbox[:, :, 0][:, [0, 0]] # (num_noobj, 2) -> duplicated target for every bounding box\n",
    "        \n",
    "        print(\"NOOBJ CONFIDENCE Loss\")\n",
    "        print(noobj_pred_confidence)\n",
    "        print(noobj_target_confidence)\n",
    "        \n",
    "        noobj_confidence_loss = lambda_noobj * F.mse_loss(noobj_pred_confidence, noobj_target_confidence, reduction=\"sum\")\n",
    "        print(noobj_confidence_loss)\n",
    "        \n",
    "        confidence_loss = obj_confidence_loss + noobj_confidence_loss\n",
    "        \n",
    "        ###\n",
    "        # Classification Loss\n",
    "        ###\n",
    "        print(\"CLASSIFICATION LOSS\")\n",
    "        obj_pred_classification = obj_pred[:, -C:] # (num_obj, C)\n",
    "        obj_target_classification = obj_target[:, -C:] # (num_obj, C)\n",
    "        classification_loss = F.mse_loss(obj_pred_classification, obj_target_classification, reduction=\"sum\")\n",
    "        \n",
    "        print(obj_pred_classification)\n",
    "        print(obj_target_classification)\n",
    "        print(classification_loss)\n",
    "        \n",
    "        # total loss\n",
    "        print(localization_loss, confidence_loss, classification_loss)\n",
    "        loss = (localization_loss + confidence_loss + classification_loss) / N\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "f7f807c0-d2d9-4137-bce3-b1ba1b05b75a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# S = 1\n",
    "# B = 2\n",
    "# C = 2\n",
    "\n",
    "S = 2\n",
    "B = 2\n",
    "C = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "5ff89d04-7e88-46e6-b0c8-2ee1bef499c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1.0000, 0.5000, 0.5000, 0.5000, 0.5000, 0.9500, 0.5000, 0.5000,\n",
      "           0.1429, 0.1429, 0.0000, 0.9500],\n",
      "          [0.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "           1.0000, 1.0000, 0.0000, 0.9500]],\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000]]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "pred = torch.zeros((S, S, 5 * B + C))\n",
    "pred[0, 0] = torch.tensor([1, 0.5, 0.5, 0.5, 0.5, 0.95, 0.5, 0.5, 1/7, 1/7, 0, 0.95])\n",
    "pred[0, 1] = torch.tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0.95])\n",
    "pred = pred.unsqueeze(0)\n",
    "pred.shape\n",
    "pred.requires_grad_()\n",
    "\n",
    "# pred = torch.zeros((2, S, S, 5 * B + C))\n",
    "# pred[0, 0, 0] = torch.tensor([1, 0.5, 0.5, 0.5, 0.5, 1, 0.5, 0.5, 1/7, 1/7, 0, 0.95])\n",
    "# pred.shape\n",
    "\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "9f00a0d5-e918-43be-8b0c-78bb6cd52ac3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1.0000, 0.5000, 0.5000, 0.1429, 0.1429, 0.0000, 1.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = torch.zeros((S, S, 5 + C))\n",
    "target[0, 0] = torch.tensor([1, 0.5, 0.5, 1/7, 1/7, 0, 1])\n",
    "target = target.unsqueeze(0)\n",
    "target.shape\n",
    "\n",
    "target.requires_grad_()\n",
    "\n",
    "# target = torch.zeros((2, S, S, 5 + C))\n",
    "# target[0, 0, 0] = torch.tensor([1, 0.5, 0.5, 1/7, 1/7, 0, 1])\n",
    "# target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "2cf35d4a-707c-4974-88bd-e4d608aba49b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CALCULATING YOLO LOSS\n",
      "BATCH SIZE: 1\n",
      "BOUNDING BOXES\n",
      "tensor([[[1.0000, 0.5000, 0.5000, 0.5000, 0.5000],\n",
      "         [0.9500, 0.5000, 0.5000, 0.1429, 0.1429]]], grad_fn=<ViewBackward0>) torch.Size([1, 2, 5])\n",
      "tensor([[[1.0000, 0.5000, 0.5000, 0.1429, 0.1429]]], grad_fn=<ViewBackward0>) torch.Size([1, 1, 5])\n",
      "OBJ PRED\n",
      "tensor([[1.0000, 0.5000, 0.5000, 0.5000, 0.5000, 0.9500, 0.5000, 0.5000, 0.1429,\n",
      "         0.1429, 0.0000, 0.9500]], grad_fn=<IndexBackward0>) torch.Size([1, 12])\n",
      "OBJ TARGET\n",
      "tensor([[1.0000, 0.5000, 0.5000, 0.1429, 0.1429, 0.0000, 1.0000]],\n",
      "       grad_fn=<IndexBackward0>) torch.Size([1, 7])\n",
      "SELECTING PREDICTOR BOXES\n",
      "RESPONSIBLE PREDICTORS\n",
      "tensor([[0.9500, 0.5000, 0.5000, 0.1429, 0.1429]], grad_fn=<ViewBackward0>) torch.Size([1, 5])\n",
      "tensor([[1.0000, 0.5000, 0.5000, 0.1429, 0.1429]], grad_fn=<SqueezeBackward1>) torch.Size([1, 5])\n",
      "BOUNDING BOX Loss\n",
      "tensor(0., grad_fn=<MulBackward0>)\n",
      "tensor(0., grad_fn=<MulBackward0>)\n",
      "CONFIDENCE Loss\n",
      "tensor([[[0., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]]], grad_fn=<ViewBackward0>) torch.Size([3, 2, 5])\n",
      "tensor([[[0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0.]]], grad_fn=<ViewBackward0>) torch.Size([3, 1, 5])\n",
      "tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "NOOBJ CONFIDENCE Loss\n",
      "tensor([[0., 1.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]], grad_fn=<SelectBackward0>)\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]], grad_fn=<IndexBackward0>)\n",
      "tensor(0.5000, grad_fn=<MulBackward0>)\n",
      "CLASSIFICATION LOSS\n",
      "tensor([[0.0000, 0.9500]], grad_fn=<SliceBackward0>)\n",
      "tensor([[0., 1.]], grad_fn=<SliceBackward0>)\n",
      "tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "tensor(0., grad_fn=<AddBackward0>) tensor(0.5025, grad_fn=<AddBackward0>) tensor(0.0025, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.5050, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yolo_loss = YOLOv1Loss(\n",
    "    S=S,\n",
    "    B=B,\n",
    "    C=C\n",
    ")\n",
    "\n",
    "loss = yolo_loss(pred, target)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "5d51aceb-f5f0-4e00-a3f8-cb02f4b9aa51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9bfd1b-eef2-40f0-9ee3-08960244eda5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ea89fc-642f-48da-9060-5485be1351a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64d7c5c-8d51-4145-ab5a-a88bdff54451",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8fd91c87-c5c5-4145-bec2-d851c8dc328f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = torch.zeros((1, 2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "fbb13c25-896a-4cda-bd4c-3cf31a1f0776",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape == torch.Size([1, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "58331274-33ca-42a2-9b1f-c72dd45c0129",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 2])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7aae12-1e91-4380-97ef-8a62499c4469",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ce5561-e706-4781-b6b7-854f849503c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
