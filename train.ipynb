{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7f27c9d-d62d-4bf4-aea7-172072cf07a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b302ee7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import dataset\n",
    "from model import ResNet18YOLOv1\n",
    "from loss import YOLOv1Loss\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecbed67-d7f8-4c72-9cfa-429f38b82556",
   "metadata": {
    "tags": []
   },
   "source": [
    "# About\n",
    "This is an implementation of YOLOv1 from ***You Only Look Once: Unified, Real-Time Object Detection by Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Firhadi.*** Object detection is figuring out what objects are in an image and where they are. Another way to look at this problem is how can we write a computer program that draws bounding boxes around objects and predicts what kind of objects they are. YOLO solves this problem and does it super fast, like state of the art fast!\n",
    "\n",
    "Let's talk about R-CNN, the predecessor to YOLO. It proposed regions, ran a classifier on every region, and did some post-processing to produce the final result. In simple language this translates to:\n",
    "1. Lemme draw a lot of bounding boxes where I think objects are\n",
    "2. Lemme figure out what are in the bounding boxes I drew\n",
    "3. Ok, I drew too many bounding boxes, lemme remove most of them and keep the important ones\n",
    "\n",
    "This is a lot of steps. What YOLO does instead is ***unified detection***. Unified detection combines the different components of object detection (where are the objects and what kind of objects are they) into one Convolutional Neural Network. You give it an image and in one swoop, it tells you exactly that.\n",
    "\n",
    "Here's how it does it:\n",
    "1. Divide the image into a SxS grid\n",
    "2. Each cell in the grid predicts B bounding boxes and C class probabilities (what it thinks the object is)\n",
    "\n",
    "We represent bounding boxes with 5 numbers: x, y, w, h, p.\n",
    "- (x, y): center of the bounding box\n",
    "- w: width\n",
    "- h: height\n",
    "- p: confidence (a measure of how confident we are that this box captures an object and matches the ground truth)\n",
    "\n",
    "Accordingly, YOLOv1 produces a SxSx(5B+C) tensor. Each cell predicts B bounding boxes, how do we choose which one is the \"true\" predictor? How do we measure how good our bounding box and classification predictions are? \n",
    "\n",
    "We check which bounding box has the greatest overlap (IOU: Intersection Over Union) with the ground truth and choose that one as a predictor. We use this loss function to measure the \"goodness\" of our predictions:\n",
    "\n",
    "![yolo loss function](https://i.stack.imgur.com/IddFu.png)\n",
    "\n",
    "On a high level, it is the squared error between our prediction and the ground truth. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2189430c-7b31-498f-8498-e5254ba79dc4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# PASCAL VOC 2007 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff97fac-bdec-4ce3-b2d6-a4d6a7945642",
   "metadata": {},
   "source": [
    "PASCAL VOC Detection Dataset contains annotated images with 20 labelled classes and bounding boxes. There are 2,501 images in the training set, 2,510 images in the validation set, and 4,952 images in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5525f8a3-d645-45ea-b44c-9f1ca9ab6c9b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: data/VOCtrainval_06-Nov-2007.tar\n",
      "Extracting data/VOCtrainval_06-Nov-2007.tar to data\n",
      "Using downloaded and verified file: data/VOCtest_06-Nov-2007.tar\n",
      "Extracting data/VOCtest_06-Nov-2007.tar to data\n"
     ]
    }
   ],
   "source": [
    "# original dataset\n",
    "pascal_voc_train = torchvision.datasets.VOCDetection(\n",
    "    root=\"data\",\n",
    "    year=\"2007\",\n",
    "    image_set=\"train\",\n",
    "    download=False\n",
    ")\n",
    "\n",
    "pascal_voc_val = torchvision.datasets.VOCDetection(\n",
    "    root=\"data\",\n",
    "    year=\"2007\",\n",
    "    image_set=\"val\",\n",
    "    download=True\n",
    ")\n",
    "\n",
    "pascal_voc_test = torchvision.datasets.VOCDetection(\n",
    "    root=\"data\",\n",
    "    year=\"2007\",\n",
    "    image_set=\"test\",\n",
    "    download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11520045",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRANSFORMING PASCAL VOC\n",
      "TRANSFORMING PASCAL VOC\n",
      "TRANSFORMING PASCAL VOC\n"
     ]
    }
   ],
   "source": [
    "# augment dataset for YOLOv1: resize and normalize image and convert bounding boxes from annotations to tensors\n",
    "voc_train = dataset.PascalVOC(pascal_voc=pascal_voc_train)\n",
    "voc_val = dataset.PascalVOC(pascal_voc=pascal_voc_val)\n",
    "voc_test = dataset.PascalVOC(pascal_voc=pascal_voc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1abb4f1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a8674b35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(voc_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(voc_val, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(voc_test, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b831392b-941b-4c64-86fa-a23bf3c558f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5f716d-75e5-4367-9fdf-fe1843843bd5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c323bd4-63e6-441d-bd8e-65b404122ba3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = \"cpu\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5065e90d-ef48-4244-92e9-2553e44518ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Hyperparameters\n",
    "- S: dimensions of SxS grid\n",
    "- B: number of bounding boxes predicted per cell\n",
    "- C: number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2f9313e8-6e96-4d4a-8dd6-98d56df27ad4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "S = 7\n",
    "B = 2\n",
    "C = 20\n",
    "lambda_coord = 5.0\n",
    "lambda_noobj = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254ef8df-55aa-4f2d-8a6b-805f5ed1b276",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8400c085-8bac-4b61-b13d-3b100a6f7c17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "yolo = ResNet18YOLOv1(S=S, B=B, C=C).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45afc591-7cce-4066-903d-60e436c74b58",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loss + Optimizer\n",
    "![yolo loss function](https://i.stack.imgur.com/IddFu.png)\n",
    "\n",
    "We use stochastic gradient descent with a learning rate of 1e-3, weight decay (L2 regularization) of 0.0005, and momentum of 0.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d36287be-210a-452c-bf59-3874c2cb6681",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "yolo_loss = YOLOv1Loss(S=S, B=B, C=C, lambda_coord=lambda_coord, lambda_noobj=lambda_noobj)\n",
    "optimizer = torch.optim.SGD(yolo.parameters(), lr=1e-3, weight_decay=0.0005, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbbfadb-3e8d-468e-9695-0e6b67b65f20",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da10513c-17e8-473b-925b-8916ddaeb8d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, train_dataloader, val_dataloader):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70b255f6-c231-474d-94e7-21cb854dcc8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b00d0ecf-1660-4057-9cd3-2b1d6f414b78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 1: 100%|█████████████████████████████| 40/40 [02:49<00:00,  4.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Train Loss: 31.714705514907838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "yolo.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i, (X, Y) in enumerate(tqdm(train_dataloader, desc=f\"Epoch {epoch + 1} of {EPOCHS}\", leave=False)):\n",
    "        X = X.to(DEVICE)\n",
    "        Y = Y.to(DEVICE)\n",
    "        \n",
    "        pred = yolo(X)\n",
    "        loss = yolo_loss(pred, Y)\n",
    "        \n",
    "        # backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    loss = total_loss / len(train_dataloader)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    print(f\"Epoch [{epoch + 1}/{EPOCHS}], Train Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e91f29-a841-48b6-9f97-434f93cf92fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning:Python",
   "language": "python",
   "name": "conda-env-deeplearning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
