{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7f27c9d-d62d-4bf4-aea7-172072cf07a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9b302ee7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import dataset\n",
    "from model import ResNet18YOLOv1\n",
    "from loss import YOLOv1Loss\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecbed67-d7f8-4c72-9cfa-429f38b82556",
   "metadata": {
    "tags": []
   },
   "source": [
    "# About\n",
    "This is an implementation of YOLOv1 from ***You Only Look Once: Unified, Real-Time Object Detection by Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Firhadi.*** Object detection is figuring out what objects are in an image and where they are. Another way to look at this problem is how can we write a computer program that draws bounding boxes around objects and predicts what kind of objects they are. YOLO solves this problem and does it super fast, like state of the art fast!\n",
    "\n",
    "Let's talk about R-CNN, the predecessor to YOLO. It proposed regions, ran a classifier on every region, and did some post-processing to produce the final result. In simple language this translates to:\n",
    "1. Lemme draw a lot of bounding boxes where I think objects are\n",
    "2. Lemme figure out what are in the bounding boxes I drew\n",
    "3. Ok, I drew too many bounding boxes, lemme remove most of them and keep the important ones\n",
    "\n",
    "This is a lot of steps. What YOLO does instead is ***unified detection***. Unified detection combines the different components of object detection (where are the objects and what kind of objects are they) into one Convolutional Neural Network. You give it an image and in one swoop, it tells you exactly that.\n",
    "\n",
    "Here's how it does it:\n",
    "1. Divide the image into a SxS grid\n",
    "2. Each cell in the grid predicts B bounding boxes and C class probabilities (what it thinks the object is)\n",
    "\n",
    "We represent bounding boxes with 5 numbers: x, y, w, h, p.\n",
    "- (x, y): center of the bounding box\n",
    "- w: width\n",
    "- h: height\n",
    "- p: confidence (a measure of how confident we are that this box captures an object and matches the ground truth)\n",
    "\n",
    "Accordingly, YOLOv1 produces a SxSx(5B+C) tensor. Each cell predicts B bounding boxes, how do we choose which one is the \"true\" predictor? How do we measure how good our bounding box and classification predictions are? \n",
    "\n",
    "We check which bounding box has the greatest overlap (IOU: Intersection Over Union) with the ground truth and choose that one as a predictor. We use this loss function to measure the \"goodness\" of our predictions:\n",
    "\n",
    "![yolo loss function](https://i.stack.imgur.com/IddFu.png)\n",
    "\n",
    "On a high level, it is the squared error between our prediction and the ground truth. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2189430c-7b31-498f-8498-e5254ba79dc4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# PASCAL VOC 2007 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff97fac-bdec-4ce3-b2d6-a4d6a7945642",
   "metadata": {},
   "source": [
    "PASCAL VOC Detection Dataset contains annotated images with 20 labelled classes and bounding boxes. There are 2,501 images in the training set, 2,510 images in the validation set, and 4,952 images in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5525f8a3-d645-45ea-b44c-9f1ca9ab6c9b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: data/VOCtrainval_06-Nov-2007.tar\n",
      "Extracting data/VOCtrainval_06-Nov-2007.tar to data\n",
      "Using downloaded and verified file: data/VOCtest_06-Nov-2007.tar\n",
      "Extracting data/VOCtest_06-Nov-2007.tar to data\n"
     ]
    }
   ],
   "source": [
    "# original dataset\n",
    "pascal_voc_train = torchvision.datasets.VOCDetection(\n",
    "    root=\"data\",\n",
    "    year=\"2007\",\n",
    "    image_set=\"train\",\n",
    "    download=False\n",
    ")\n",
    "\n",
    "pascal_voc_val = torchvision.datasets.VOCDetection(\n",
    "    root=\"data\",\n",
    "    year=\"2007\",\n",
    "    image_set=\"val\",\n",
    "    download=True\n",
    ")\n",
    "\n",
    "pascal_voc_test = torchvision.datasets.VOCDetection(\n",
    "    root=\"data\",\n",
    "    year=\"2007\",\n",
    "    image_set=\"test\",\n",
    "    download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "11520045",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRANSFORMING PASCAL VOC\n",
      "TRANSFORMING PASCAL VOC\n",
      "TRANSFORMING PASCAL VOC\n"
     ]
    }
   ],
   "source": [
    "# augment dataset for YOLOv1: resize and normalize image and convert bounding boxes from annotations to tensors\n",
    "voc_train = dataset.PascalVOC(pascal_voc=pascal_voc_train)\n",
    "voc_val = dataset.PascalVOC(pascal_voc=pascal_voc_val)\n",
    "voc_test = dataset.PascalVOC(pascal_voc=pascal_voc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1abb4f1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a8674b35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(voc_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(voc_val, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(voc_test, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b831392b-941b-4c64-86fa-a23bf3c558f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5f716d-75e5-4367-9fdf-fe1843843bd5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5c323bd4-63e6-441d-bd8e-65b404122ba3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = \"cpu\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5065e90d-ef48-4244-92e9-2553e44518ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Hyperparameters\n",
    "- S: dimensions of SxS grid\n",
    "- B: number of bounding boxes predicted per cell\n",
    "- C: number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2f9313e8-6e96-4d4a-8dd6-98d56df27ad4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "S = 7\n",
    "B = 2\n",
    "C = 20\n",
    "lambda_coord = 5.0\n",
    "lambda_noobj = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254ef8df-55aa-4f2d-8a6b-805f5ed1b276",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model\n",
    "ResNet18 convolutional layers pretrained on ImageNet with 2 feedforward layers outputting a (N x S x S x (5B + C)) tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8400c085-8bac-4b61-b13d-3b100a6f7c17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "yolo = ResNet18YOLOv1(S=S, B=B, C=C).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45afc591-7cce-4066-903d-60e436c74b58",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loss + Optimizer\n",
    "![yolo loss function](https://i.stack.imgur.com/IddFu.png)\n",
    "\n",
    "We use stochastic gradient descent with a learning rate of 1e-3, weight decay (L2 regularization) of 0.0005, and momentum of 0.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d36287be-210a-452c-bf59-3874c2cb6681",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "yolo_loss = YOLOv1Loss(S=S, B=B, C=C, lambda_coord=lambda_coord, lambda_noobj=lambda_noobj)\n",
    "optimizer = torch.optim.SGD(yolo.parameters(), lr=1e-3, weight_decay=0.0005, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbbfadb-3e8d-468e-9695-0e6b67b65f20",
   "metadata": {},
   "source": [
    "## Train\n",
    "Train model with a learning rate of 1e-3 for the first few epochs, raise learning rate to 1e-2 and train for 75 epochs, then lower to 1e-3 for 30 epochs, and finally 1e-4 for 30 epochs.\n",
    "\n",
    "We train the network for about 135 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "da10513c-17e8-473b-925b-8916ddaeb8d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FIRST_EPOCHS = 3\n",
    "EPOCHS = 9\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 0.0005\n",
    "\n",
    "def train_yolo(model, criterion, train_dataloader, val_dataloader):\n",
    "    pre_optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, weight_decay=WEIGHT_DECAY, momentum=MOMENTUM)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, weight_decay=WEIGHT_DECAY, momentum=MOMENTUM)\n",
    "    scheduler = MultiStepLR(optimizer,\n",
    "                            milestones=[3, 6],\n",
    "                            gamma=0.1)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    N = len(train_dataloader)\n",
    "    \n",
    "    # pretrain lr=1e-3\n",
    "    for epoch in range(FIRST_EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for i, (X, Y) in enumerate(tqdm(train_dataloader, leave=False, desc=f\"Pretrain Epoch [{epoch+1}/{FIRST_EPOCHS}]\")):\n",
    "            X = X.to(DEVICE)\n",
    "            Y = Y.to(DEVICE)\n",
    "            \n",
    "            pred = model(X)\n",
    "            loss = criterion(pred, Y)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # backprop\n",
    "            pre_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            pre_optimizer.step()\n",
    "            break\n",
    "        \n",
    "        loss = total_loss\n",
    "        # loss = total_loss / N\n",
    "        print(f\"Pretrain Epoch [{epoch+1}/{FIRST_EPOCHS}]: Train Loss={loss}\")\n",
    "            \n",
    "    # train\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        for i, (X, Y) in enumerate(tqdm(train_dataloader, leave=False, desc=f\"Epoch [{epoch+1}/{FIRST_EPOCHS}]: lr={lr}\")):\n",
    "            X = X.to(DEVICE)\n",
    "            Y = Y.to(DEVICE)\n",
    "            \n",
    "            pred = model(X)\n",
    "            loss = criterion(pred, Y)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            break\n",
    "            \n",
    "        train_loss = total_loss\n",
    "        # loss = train_loss / N\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # evaluate on validation set\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X, Y in val_dataloader:\n",
    "                X = X.to(DEVICE)\n",
    "                Y = Y.to(DEVICE)\n",
    "                \n",
    "                pred = model(X)\n",
    "                loss = criterion(pred, Y)\n",
    "                val_loss += loss.item()\n",
    "                break\n",
    "            \n",
    "        val_loss = val_loss\n",
    "        # val_loss = val_loss / len(val_dataloader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}]: Train Loss={train_loss}, Val Loss={val_loss}\")\n",
    "    \n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "40116bcc-8aef-427f-8498-d0c43cc93077",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6ebe2f5555e447186ca351b5d41cb02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pretrain Epoch [1/3]:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch [1/3]: Train Loss=30.060768127441406\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bf92c27bf504b529296e813b99fed83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pretrain Epoch [2/3]:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch [2/3]: Train Loss=28.406982421875\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26bcdf6ece374fd1b98a16e046531d86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pretrain Epoch [3/3]:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch [3/3]: Train Loss=25.85401153564453\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d757413c8534d38a2f267f747b525de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch [1/3]: lr=0.01:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/9]: Train Loss=24.73212432861328, Val Loss=14.703189849853516\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ae7a9f1a17c464f9075a30b5dfbcd89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch [2/3]: lr=0.01:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/9]: Train Loss=17.511695861816406, Val Loss=12.124261856079102\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "345a656832b04407a9556ce2e5a8c7d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch [3/3]: lr=0.01:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/9]: Train Loss=13.070474624633789, Val Loss=13.810590744018555\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4fc9b5157a34472831ed774da0221e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch [4/3]: lr=0.001:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/9]: Train Loss=13.145705223083496, Val Loss=12.526182174682617\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f995964ce83f478cabb430e809b8ffb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch [5/3]: lr=0.001:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/9]: Train Loss=15.60600471496582, Val Loss=14.99415397644043\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2abee4d98f9b48d7b74d9ccc90e12b44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch [6/3]: lr=0.001:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/9]: Train Loss=15.620427131652832, Val Loss=14.155112266540527\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c58424a1551498eb9d3004a1053f0b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch [7/3]: lr=0.0001:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/9]: Train Loss=12.948392868041992, Val Loss=12.987738609313965\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ffc4e6dab2d4c0aa4a816892bb22482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch [8/3]: lr=0.0001:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_losses, val_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_yolo\u001b[49m\u001b[43m(\u001b[49m\u001b[43myolo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m           \u001b[49m\u001b[43myolo_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m           \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m           \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[85], line 73\u001b[0m, in \u001b[0;36mtrain_yolo\u001b[0;34m(model, criterion, train_dataloader, val_dataloader)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m X, Y \u001b[38;5;129;01min\u001b[39;00m val_dataloader:\n\u001b[0;32m---> 73\u001b[0m         X \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m         Y \u001b[38;5;241m=\u001b[39m Y\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     76\u001b[0m         pred \u001b[38;5;241m=\u001b[39m model(X)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses, val_losses = train_yolo(yolo,\n",
    "           yolo_loss,\n",
    "           train_dataloader=train_dataloader,\n",
    "           val_dataloader=val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e91f29-a841-48b6-9f97-434f93cf92fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
