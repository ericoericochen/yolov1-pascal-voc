{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7f27c9d-d62d-4bf4-aea7-172072cf07a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9b302ee7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import dataset\n",
    "from model import ResNet18YOLOv1\n",
    "from loss import YOLOv1Loss\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecbed67-d7f8-4c72-9cfa-429f38b82556",
   "metadata": {
    "tags": []
   },
   "source": [
    "# About\n",
    "This is an implementation of YOLOv1 from ***You Only Look Once: Unified, Real-Time Object Detection by Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Firhadi.*** Object detection is figuring out what objects are in an image and where they are. Another way to look at this problem is how can we write a computer program that draws bounding boxes around objects and predicts what kind of objects they are. YOLO solves this problem and does it super fast, like state of the art fast!\n",
    "\n",
    "Let's talk about R-CNN, the predecessor to YOLO. It proposed regions, ran a classifier on every region, and did some post-processing to produce the final result. In simple language this translates to:\n",
    "1. Lemme draw a lot of bounding boxes where I think objects are\n",
    "2. Lemme figure out what are in the bounding boxes I drew\n",
    "3. Ok, I drew too many bounding boxes, lemme remove most of them and keep the important ones\n",
    "\n",
    "This is a lot of steps. What YOLO does instead is ***unified detection***. Unified detection combines the different components of object detection (where are the objects and what kind of objects are they) into one Convolutional Neural Network. You give it an image and in one swoop, it tells you exactly that.\n",
    "\n",
    "Here's how it does it:\n",
    "1. Divide the image into a SxS grid\n",
    "2. Each cell in the grid predicts B bounding boxes and C class probabilities (what it thinks the object is)\n",
    "\n",
    "We represent bounding boxes with 5 numbers: x, y, w, h, p.\n",
    "- (x, y): center of the bounding box\n",
    "- w: width\n",
    "- h: height\n",
    "- p: confidence (a measure of how confident we are that this box captures an object and matches the ground truth)\n",
    "\n",
    "Accordingly, YOLOv1 produces a SxSx(5B+C) tensor. Each cell predicts B bounding boxes, how do we choose which one is the \"true\" predictor? How do we measure how good our bounding box and classification predictions are? \n",
    "\n",
    "We check which bounding box has the greatest overlap (IOU: Intersection Over Union) with the ground truth and choose that one as a predictor. We use this loss function to measure the \"goodness\" of our predictions:\n",
    "\n",
    "![yolo loss function](https://i.stack.imgur.com/IddFu.png)\n",
    "\n",
    "On a high level, it is the squared error between our prediction and the ground truth. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2189430c-7b31-498f-8498-e5254ba79dc4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# PASCAL VOC 2007 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff97fac-bdec-4ce3-b2d6-a4d6a7945642",
   "metadata": {},
   "source": [
    "PASCAL VOC Detection Dataset contains annotated images with 20 labelled classes and bounding boxes. There are 2,501 images in the training set, 2,510 images in the validation set, and 4,952 images in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5525f8a3-d645-45ea-b44c-9f1ca9ab6c9b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: data/VOCtrainval_06-Nov-2007.tar\n",
      "Extracting data/VOCtrainval_06-Nov-2007.tar to data\n",
      "Using downloaded and verified file: data/VOCtest_06-Nov-2007.tar\n",
      "Extracting data/VOCtest_06-Nov-2007.tar to data\n"
     ]
    }
   ],
   "source": [
    "# original dataset\n",
    "pascal_voc_train = torchvision.datasets.VOCDetection(\n",
    "    root=\"data\",\n",
    "    year=\"2007\",\n",
    "    image_set=\"train\",\n",
    "    download=False\n",
    ")\n",
    "\n",
    "pascal_voc_val = torchvision.datasets.VOCDetection(\n",
    "    root=\"data\",\n",
    "    year=\"2007\",\n",
    "    image_set=\"val\",\n",
    "    download=True\n",
    ")\n",
    "\n",
    "pascal_voc_test = torchvision.datasets.VOCDetection(\n",
    "    root=\"data\",\n",
    "    year=\"2007\",\n",
    "    image_set=\"test\",\n",
    "    download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "11520045",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRANSFORMING PASCAL VOC\n",
      "TRANSFORMING PASCAL VOC\n",
      "TRANSFORMING PASCAL VOC\n"
     ]
    }
   ],
   "source": [
    "# augment dataset for YOLOv1: resize and normalize image and convert bounding boxes from annotations to tensors\n",
    "voc_train = dataset.PascalVOC(pascal_voc=pascal_voc_train)\n",
    "voc_val = dataset.PascalVOC(pascal_voc=pascal_voc_val)\n",
    "voc_test = dataset.PascalVOC(pascal_voc=pascal_voc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1abb4f1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a8674b35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(voc_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(voc_val, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(voc_test, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b831392b-941b-4c64-86fa-a23bf3c558f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5f716d-75e5-4367-9fdf-fe1843843bd5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5c323bd4-63e6-441d-bd8e-65b404122ba3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = \"cpu\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5065e90d-ef48-4244-92e9-2553e44518ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Hyperparameters\n",
    "- S: dimensions of SxS grid\n",
    "- B: number of bounding boxes predicted per cell\n",
    "- C: number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2f9313e8-6e96-4d4a-8dd6-98d56df27ad4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "S = 7\n",
    "B = 2\n",
    "C = 20\n",
    "lambda_coord = 5.0\n",
    "lambda_noobj = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254ef8df-55aa-4f2d-8a6b-805f5ed1b276",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model\n",
    "ResNet18 convolutional layers pretrained on ImageNet with 2 feedforward layers outputting a (N x S x S x (5B + C)) tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8400c085-8bac-4b61-b13d-3b100a6f7c17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "yolo = ResNet18YOLOv1(S=S, B=B, C=C).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45afc591-7cce-4066-903d-60e436c74b58",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loss + Optimizer\n",
    "![yolo loss function](https://i.stack.imgur.com/IddFu.png)\n",
    "\n",
    "We use stochastic gradient descent with a learning rate of 1e-3, weight decay (L2 regularization) of 0.0005, and momentum of 0.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d36287be-210a-452c-bf59-3874c2cb6681",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "yolo_loss = YOLOv1Loss(S=S, B=B, C=C, lambda_coord=lambda_coord, lambda_noobj=lambda_noobj)\n",
    "optimizer = torch.optim.SGD(yolo.parameters(), lr=1e-3, weight_decay=0.0005, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbbfadb-3e8d-468e-9695-0e6b67b65f20",
   "metadata": {},
   "source": [
    "## Train\n",
    "Train model with a learning rate of 1e-3 for the first few epochs, raise learning rate to 1e-2 and train for 75 epochs, then lower to 1e-3 for 30 epochs, and finally 1e-4 for 30 epochs.\n",
    "\n",
    "We train the network for about 135 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cc42c578-7fd1-462a-b415-c45d8529573e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_loss(model, criterion, dataloader):\n",
    "    total_loss = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, Y in dataloader:\n",
    "            X = X.to(DEVICE)\n",
    "            Y = Y.to(DEVICE)\n",
    "\n",
    "            pred = model(X)\n",
    "            loss = criterion(pred, Y)\n",
    "            total_loss += loss.item()\n",
    "            break\n",
    "            \n",
    "    N = len(dataloader)\n",
    "    loss = total_loss / N\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "da10513c-17e8-473b-925b-8916ddaeb8d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FIRST_EPOCHS = 3\n",
    "EPOCHS = 9\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 0.0005\n",
    "\n",
    "def train_yolo(model, criterion, train_dataloader, val_dataloader):\n",
    "    pre_optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, weight_decay=WEIGHT_DECAY, momentum=MOMENTUM)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, weight_decay=WEIGHT_DECAY, momentum=MOMENTUM)\n",
    "    scheduler = MultiStepLR(optimizer,\n",
    "                            milestones=[3, 6],\n",
    "                            gamma=0.1)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    N = len(train_dataloader)\n",
    "    \n",
    "    # pretrain lr=1e-3\n",
    "    for epoch in range(FIRST_EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for i, (X, Y) in enumerate(tqdm(train_dataloader, leave=False, desc=f\"Pretrain Epoch [{epoch+1}/{FIRST_EPOCHS}]\")):\n",
    "            X = X.to(DEVICE)\n",
    "            Y = Y.to(DEVICE)\n",
    "            \n",
    "            pred = model(X)\n",
    "            loss = criterion(pred, Y)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # backprop\n",
    "            pre_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            pre_optimizer.step()\n",
    "            break\n",
    "        \n",
    "        loss = total_loss\n",
    "        # loss = total_loss / N\n",
    "        print(f\"Pretrain Epoch [{epoch+1}/{FIRST_EPOCHS}]: Train Loss={loss}\")\n",
    "            \n",
    "    # train\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        for i, (X, Y) in enumerate(tqdm(train_dataloader, leave=False, desc=f\"Epoch [{epoch+1}/{FIRST_EPOCHS}]: lr={lr}\")):\n",
    "            X = X.to(DEVICE)\n",
    "            Y = Y.to(DEVICE)\n",
    "            \n",
    "            pred = model(X)\n",
    "            loss = criterion(pred, Y)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            break\n",
    "            \n",
    "        train_loss = total_loss\n",
    "        # loss = train_loss / N\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # evaluate on validation set\n",
    "        val_loss = evaluate_loss(model, criterion, val_dataloader)\n",
    "#         model.eval()\n",
    "#         val_loss = 0\n",
    "#         with torch.no_grad():\n",
    "#             for X, Y in val_dataloader:\n",
    "#                 X = X.to(DEVICE)\n",
    "#                 Y = Y.to(DEVICE)\n",
    "                \n",
    "#                 pred = model(X)\n",
    "#                 loss = criterion(pred, Y)\n",
    "#                 val_loss += loss.item()\n",
    "#                 break\n",
    "            \n",
    "        val_loss = val_loss\n",
    "        # val_loss = val_loss / len(val_dataloader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}]: Train Loss={train_loss}, Val Loss={val_loss}\")\n",
    "    \n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "40116bcc-8aef-427f-8498-d0c43cc93077",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0db56170829411bb2c38bcd1334c1d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pretrain Epoch [1/3]:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch [1/3]: Train Loss=13.933432579040527\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd4d43ae3277493b9e81ddcae83174db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pretrain Epoch [2/3]:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch [2/3]: Train Loss=16.80512809753418\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4689235670f540cfb4f0a0c6c82378a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pretrain Epoch [3/3]:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch [3/3]: Train Loss=14.985215187072754\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8f68833815842fba9a776ea7fc621cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch [1/3]: lr=0.01:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/9]: Train Loss=14.813060760498047, Val Loss=0.32674336433410645\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a7abcfa16ef40d9b1b93a22a9a1ad92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch [2/3]: lr=0.01:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/9]: Train Loss=19.093591690063477, Val Loss=0.3061770677566528\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c6982158858449b8ae97f64b1861b8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch [3/3]: lr=0.01:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/9]: Train Loss=17.033845901489258, Val Loss=0.28183751106262206\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18c473c7306842008f94996b45ca38a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch [4/3]: lr=0.001:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_losses, val_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_yolo\u001b[49m\u001b[43m(\u001b[49m\u001b[43myolo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m           \u001b[49m\u001b[43myolo_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m           \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m           \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[90], line 56\u001b[0m, in \u001b[0;36mtrain_yolo\u001b[0;34m(model, criterion, train_dataloader, val_dataloader)\u001b[0m\n\u001b[1;32m     54\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     55\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 56\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     60\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m total_loss\n",
      "File \u001b[0;32m~/miniconda3/envs/ai/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:69\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     68\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ai/lib/python3.9/site-packages/torch/optim/optimizer.py:293\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    290\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    291\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 293\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    296\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ai/lib/python3.9/site-packages/torch/optim/optimizer.py:36\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 36\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/miniconda3/envs/ai/lib/python3.9/site-packages/torch/optim/sgd.py:75\u001b[0m, in \u001b[0;36mSGD.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     71\u001b[0m momentum_buffer_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     73\u001b[0m has_sparse_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(group, params_with_grad, d_p_list, momentum_buffer_list)\n\u001b[0;32m---> 75\u001b[0m \u001b[43msgd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43md_p_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum_buffer_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmomentum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdampening\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdampening\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnesterov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnesterov\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# update momentum_buffers in state\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p, momentum_buffer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(params_with_grad, momentum_buffer_list):\n",
      "File \u001b[0;32m~/miniconda3/envs/ai/lib/python3.9/site-packages/torch/optim/sgd.py:221\u001b[0m, in \u001b[0;36msgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, has_sparse_grad, foreach, weight_decay, momentum, lr, dampening, nesterov, maximize)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    219\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_sgd\n\u001b[0;32m--> 221\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m     \u001b[49m\u001b[43md_p_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmomentum_buffer_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdampening\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdampening\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m     \u001b[49m\u001b[43mnesterov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnesterov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ai/lib/python3.9/site-packages/torch/optim/sgd.py:257\u001b[0m, in \u001b[0;36m_single_tensor_sgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, weight_decay, momentum, lr, dampening, nesterov, maximize, has_sparse_grad)\u001b[0m\n\u001b[1;32m    255\u001b[0m     momentum_buffer_list[i] \u001b[38;5;241m=\u001b[39m buf\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 257\u001b[0m     \u001b[43mbuf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39madd_(d_p, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m dampening)\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nesterov:\n\u001b[1;32m    260\u001b[0m     d_p \u001b[38;5;241m=\u001b[39m d_p\u001b[38;5;241m.\u001b[39madd(buf, alpha\u001b[38;5;241m=\u001b[39mmomentum)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses, val_losses = train_yolo(yolo,\n",
    "           yolo_loss,\n",
    "           train_dataloader=train_dataloader,\n",
    "           val_dataloader=val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e91f29-a841-48b6-9f97-434f93cf92fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
