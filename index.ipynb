{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1f08891c-29e2-430f-882c-3d6c0f0eb1a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "afc3e00b-aa61-494e-a841-1403c91ad8e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import dataset\n",
    "from model import ResNet18YOLOv1\n",
    "from loss import YOLOv1Loss\n",
    "from tqdm import tqdm\n",
    "from evaluate import get_bboxes, mean_average_precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6ab21d-6f8d-485c-994d-8f603994f0ae",
   "metadata": {},
   "source": [
    "# About\n",
    "This is an implementation of YOLOv1 from ***You Only Look Once: Unified, Real-Time Object Detection by Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Firhadi.*** Object detection is figuring out what objects are in an image and where they are. Another way to look at this problem is how can we write a computer program that draws bounding boxes around objects and predicts what kind of objects they are. YOLO solves this problem and does it super fast, like state of the art fast! I made slight modifications to the architecture and loss function which I'll discuss further down.\n",
    "\n",
    "Let's talk about R-CNN, the predecessor to YOLO. It proposed regions, ran a classifier on every region, and did some post-processing to produce the final result. In simple language this translates to:\n",
    "1. Lemme draw a lot of bounding boxes where I think objects are\n",
    "2. Lemme figure out what are in the bounding boxes I drew\n",
    "3. Ok, I drew too many bounding boxes, lemme remove most of them and keep the important ones\n",
    "\n",
    "This is a lot of steps. What YOLO does instead is ***unified detection***. Unified detection combines the different components of object detection (where are the objects and what kind of objects are they) into one Convolutional Neural Network. You give it an image and in one swoop, it tells you exactly that.\n",
    "\n",
    "Here's how it does it:\n",
    "1. Divide the image into a SxS grid\n",
    "2. Each cell in the grid predicts B bounding boxes and C class probabilities (what it thinks the object is)\n",
    "\n",
    "We represent bounding boxes with 5 numbers: x, y, w, h, p.\n",
    "- (x, y): center of the bounding box\n",
    "- w: width\n",
    "- h: height\n",
    "- p: confidence (a measure of how confident we are that this box captures an object and matches the ground truth)\n",
    "\n",
    "Accordingly, YOLOv1 produces a SxSx(5B+C) tensor. Each cell predicts B bounding boxes, how do we choose which one is the \"true\" predictor? How do we measure how good our bounding box and classification predictions are? \n",
    "\n",
    "We check which bounding box has the greatest overlap (IOU: Intersection Over Union) with the ground truth and choose that one as a predictor. We use this loss function to measure the \"goodness\" of our predictions:\n",
    "\n",
    "![yolo loss function](https://i.stack.imgur.com/IddFu.png)\n",
    "\n",
    "On a high level, it is the squared error between our prediction and the ground truth. Let's start training!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0990c1-443d-4ef6-815b-85219f362da7",
   "metadata": {},
   "source": [
    "# Data (PASCAL VOC 2007)\n",
    "PASCAL VOC Detection Dataset contains annotated images with 20 labelled classes and bounding boxes. There are 2,501 images in the training set, 2,510 images in the validation set, and 4,952 images in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9426d062-f1ea-4666-9828-555238cce5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original dataset\n",
    "pascal_voc_train = torchvision.datasets.VOCDetection(\n",
    "    root=\"data\",\n",
    "    year=\"2007\",\n",
    "    image_set=\"train\",\n",
    "    download=False\n",
    ")\n",
    "\n",
    "pascal_voc_val = torchvision.datasets.VOCDetection(\n",
    "    root=\"data\",\n",
    "    year=\"2007\",\n",
    "    image_set=\"val\",\n",
    "    download=False\n",
    ")\n",
    "\n",
    "pascal_voc_test = torchvision.datasets.VOCDetection(\n",
    "    root=\"data\",\n",
    "    year=\"2007\",\n",
    "    image_set=\"test\",\n",
    "    download=False\n",
    ")\n",
    "\n",
    "# resize to 448x448, normalize, and convert annotations to target tensors\n",
    "voc_train = dataset.PascalVOC(pascal_voc=pascal_voc_train)\n",
    "voc_val = dataset.PascalVOC(pascal_voc=pascal_voc_val)\n",
    "voc_test = dataset.PascalVOC(pascal_voc=pascal_voc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "dff579e9-dc2d-42ab-9941-fbfa7aad7c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_loader = DataLoader(voc_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(voc_val, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(voc_test, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b19bf5ea-e1a5-4c45-883b-5fb57eb7de42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 448, 448]) torch.Size([64, 7, 7, 25])\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd349501-c0d8-4f17-b193-aa5e46958439",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c0779b53-46a7-4e7a-9e72-e335c5a70cf0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = \"cpu\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165ccdf4-9c4d-4a3c-a070-70d8d38dc370",
   "metadata": {},
   "source": [
    "# Hyperparameters\n",
    "- S: dimensions of SxS grid\n",
    "- B: number of bounding boxes predicted per cell\n",
    "- C: number of classes\n",
    "- lambda_coord: penalty on incorrect localization loss\n",
    "- lambda_noobj: penalty on incorrect noobj confidence loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "acc9e47e-6e57-46e5-8c03-b533c891426c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "S = 7\n",
    "B = 2\n",
    "C = 20\n",
    "lambda_coord = 5.0\n",
    "lambda_noobj = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bec5d95-0199-4552-95c7-4b6d97980fc4",
   "metadata": {},
   "source": [
    "# Training Setup\n",
    "Model, loss function, optimizer, scheduler and evaluation utils\n",
    "\n",
    "- Model: ResNet18 convolutional layers pretrained on ImageNet with 2 feedforward layers outputting a (N x S x S x (5B + C)) tensor\n",
    "- Loss: Squared Error Loss\n",
    "- Optimizer + Scheduler: Stochastic Gradient Descent with momentum of 0.9 and weight decay of 0.0005. We train with learning rate set to 1e-3 for the first 75 epochs, 1e-4 for the next 30 epochs, and 1e-5 for the final 30 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ba24ea55-5d84-4847-9304-e39e5417c712",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "yolo = ResNet18YOLOv1(S=S, B=B, C=C).to(DEVICE)\n",
    "yolo_loss = YOLOv1Loss(S=S, B=B, C=C, lambda_coord=lambda_coord, lambda_noobj=lambda_noobj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3d7944b6-02d3-4492-be2b-c9c9d4d5db41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 0.0005\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "optimizer = torch.optim.SGD(yolo.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = MultiStepLR(optimizer,\n",
    "                        milestones=[75, 105],\n",
    "                        gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55178899-b207-43a0-b351-d8508b46a2f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluating Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "762600af-25e1-4a7a-af61-a2914c892db0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_loss(model, criterion, dataloader):\n",
    "    total_loss = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, Y in dataloader:\n",
    "            X = X.to(DEVICE)\n",
    "            Y = Y.to(DEVICE)\n",
    "\n",
    "            pred = model(X)\n",
    "            loss = criterion(pred, Y)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "    N = len(dataloader)\n",
    "    loss = total_loss / N\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fadd2fd-d7b5-467a-913d-84a7442e1e22",
   "metadata": {},
   "source": [
    "## Evaluating mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "50f98021-62a9-4213-b921-ab77362527ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CONFIDENCE_THRESHOLD = 0.5\n",
    "IOU_THRESHOLD = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3a01196e-993e-40a4-97e9-04d9c471aca0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bboxes(model, dataloader):\n",
    "    model.eval()\n",
    "    pred_bboxes = []\n",
    "    target_bboxes = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, Y in dataloader:\n",
    "            X, Y = X.to(DEVICE), Y.to(DEVICE)\n",
    "            pred = model(X)\n",
    "            \n",
    "            for i in range(len(X)):\n",
    "                x = pred[i]\n",
    "                y = Y[i]\n",
    "                \n",
    "                pred_bbox = get_bboxes(x, confidence_threshold=CONFIDENCE_THRESHOLD, iou_threshold=IOU_THRESHOLD, S=S, B=B, C=C)\n",
    "                target_bbox = get_bboxes(y, confidence_threshold=CONFIDENCE_THRESHOLD, iou_threshold=IOU_THRESHOLD, S=S, B=B, C=C)\n",
    "                \n",
    "                pred_bboxes.append(pred_bbox)\n",
    "                target_bboxes.append(target_bbox)\n",
    "    \n",
    "    return pred_bboxes, target_bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "dc8fec1a-683f-45b7-82d9-cb0aac3ffedb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_mAP(model, dataloader):\n",
    "    pred_bboxes, target_bboxes = bboxes(model, dataloader)\n",
    "    mAP = mean_average_precision(pred_bboxes, target_bboxes, iou_threshold=IOU_THRESHOLD, C=C)\n",
    "    \n",
    "    return mAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b524fa7b-cbca-43af-8efd-c6334c553433",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5430c9cb-2383-4030-b7c5-3e561e51aa3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1dc6eb43-0652-4fc0-8d22-f7720c24bc7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EPOCHS = 135\n",
    "\n",
    "def train(model, criterion, train_loader, val_loader, optimizer, scheduler):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_mAPs = []\n",
    "    \n",
    "    best_mAP = -float(\"inf\")\n",
    "    \n",
    "    N = len(train_loader)\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        # set to train mode\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        pbar = tqdm(train_loader, leave=False, desc=f\"Epoch [{epoch+1}/{EPOCHS}]: lr={lr}\")\n",
    "        \n",
    "        for i, (X, Y) in enumerate(pbar):\n",
    "            X, Y = X.to(DEVICE), Y.to(DEVICE)\n",
    "            pred = model(X)\n",
    "            loss = criterion(pred, Y)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # update progress bar\n",
    "            pbar.set_postfix(batch_loss=loss.item())\n",
    "        \n",
    "        # update learning rate with scheduler\n",
    "        scheduler.step()  \n",
    "        \n",
    "        # calculate metrics\n",
    "        train_loss = total_loss / N\n",
    "        val_loss = compute_loss(model, criterion, val_loader)\n",
    "        val_mAP = compute_mAP(model, val_loader)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        val_mAPs.append(val_mAP)\n",
    "        \n",
    "        if val_mAP > best_mAP:\n",
    "            best_mAP = val_mAP\n",
    "            \n",
    "            # save best model\n",
    "            print(\"=> saving best model\")\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}]: Loss={train_loss}, Val Loss={val_loss}, mAP={val_mAP}\")\n",
    "    \n",
    "    return {\n",
    "        \"train_losses\": train_losses,\n",
    "        \"val_losses\": val_losses,\n",
    "        \"val_mAP\": val_mAP,\n",
    "        \"epochs\": EPOCHS\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "822011ce-19f5-4497-9efb-bc465004e0fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[114], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_result \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43myolo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m      \u001b[49m\u001b[43myolo_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m      \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m      \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m      \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[113], line 28\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, criterion, train_loader, val_loader, optimizer, scheduler)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# backprop\u001b[39;00m\n\u001b[1;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 28\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# update progress bar\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ai/lib/python3.9/site-packages/torch/_tensor.py:491\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    483\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    484\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    489\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    490\u001b[0m     )\n\u001b[0;32m--> 491\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ai/lib/python3.9/site-packages/torch/autograd/__init__.py:204\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_result = train(yolo, \n",
    "      yolo_loss, \n",
    "      train_loader=train_loader, \n",
    "      val_loader=val_loader, \n",
    "      optimizer=optimizer, \n",
    "      scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cd14f5cd-d313-4de5-9035-22fead0a7930",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_losses': [], 'val_losses': [], 'val_mAP': [], 'epochs': 135}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b841a7-200d-464d-a6d5-23178acf7aca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
