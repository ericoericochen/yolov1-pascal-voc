{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f08891c-29e2-430f-882c-3d6c0f0eb1a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afc3e00b-aa61-494e-a841-1403c91ad8e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import dataset\n",
    "from model import ResNet18YOLOv1\n",
    "from loss import YOLOv1Loss\n",
    "from tqdm import tqdm\n",
    "from evaluate import get_bboxes, mean_average_precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6ab21d-6f8d-485c-994d-8f603994f0ae",
   "metadata": {},
   "source": [
    "# About\n",
    "This is an implementation of YOLOv1 from ***You Only Look Once: Unified, Real-Time Object Detection by Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Firhadi.*** Object detection is figuring out what objects are in an image and where they are. Another way to look at this problem is how can we write a computer program that draws bounding boxes around objects and predicts what kind of objects they are. YOLO solves this problem and does it super fast, like state of the art fast! I made slight modifications to the architecture and loss function which I'll discuss further down.\n",
    "\n",
    "Let's talk about R-CNN, the predecessor to YOLO. It proposed regions, ran a classifier on every region, and did some post-processing to produce the final result. In simple language this translates to:\n",
    "1. Lemme draw a lot of bounding boxes where I think objects are\n",
    "2. Lemme figure out what are in the bounding boxes I drew\n",
    "3. Ok, I drew too many bounding boxes, lemme remove most of them and keep the important ones\n",
    "\n",
    "This is a lot of steps. What YOLO does instead is ***unified detection***. Unified detection combines the different components of object detection (where are the objects and what kind of objects are they) into one Convolutional Neural Network. You give it an image and in one swoop, it tells you exactly that.\n",
    "\n",
    "Here's how it does it:\n",
    "1. Divide the image into a SxS grid\n",
    "2. Each cell in the grid predicts B bounding boxes and C class probabilities (what it thinks the object is)\n",
    "\n",
    "We represent bounding boxes with 5 numbers: x, y, w, h, p.\n",
    "- (x, y): center of the bounding box\n",
    "- w: width\n",
    "- h: height\n",
    "- p: confidence (a measure of how confident we are that this box captures an object and matches the ground truth)\n",
    "\n",
    "Accordingly, YOLOv1 produces a SxSx(5B+C) tensor. Each cell predicts B bounding boxes, how do we choose which one is the \"true\" predictor? How do we measure how good our bounding box and classification predictions are? \n",
    "\n",
    "We check which bounding box has the greatest overlap (IOU: Intersection Over Union) with the ground truth and choose that one as a predictor. We use this loss function to measure the \"goodness\" of our predictions:\n",
    "\n",
    "![yolo loss function](https://i.stack.imgur.com/IddFu.png)\n",
    "\n",
    "On a high level, it is the squared error between our prediction and the ground truth. Let's start training!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0990c1-443d-4ef6-815b-85219f362da7",
   "metadata": {},
   "source": [
    "# Data (PASCAL VOC 2007)\n",
    "PASCAL VOC Detection Dataset contains annotated images with 20 labelled classes and bounding boxes. There are 2,501 images in the training set, 2,510 images in the validation set, and 4,952 images in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9426d062-f1ea-4666-9828-555238cce5db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# original dataset\n",
    "pascal_voc_train = torchvision.datasets.VOCDetection(\n",
    "    root=\"data\",\n",
    "    year=\"2007\",\n",
    "    image_set=\"train\",\n",
    "    download=False\n",
    ")\n",
    "\n",
    "pascal_voc_val = torchvision.datasets.VOCDetection(\n",
    "    root=\"data\",\n",
    "    year=\"2007\",\n",
    "    image_set=\"val\",\n",
    "    download=False\n",
    ")\n",
    "\n",
    "pascal_voc_test = torchvision.datasets.VOCDetection(\n",
    "    root=\"data\",\n",
    "    year=\"2007\",\n",
    "    image_set=\"test\",\n",
    "    download=False\n",
    ")\n",
    "\n",
    "# resize to 448x448, normalize, and convert annotations to target tensors\n",
    "voc_train = dataset.PascalVOC(pascal_voc=pascal_voc_train)\n",
    "voc_val = dataset.PascalVOC(pascal_voc=pascal_voc_val)\n",
    "voc_test = dataset.PascalVOC(pascal_voc=pascal_voc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dff579e9-dc2d-42ab-9941-fbfa7aad7c05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataloaders\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_loader = DataLoader(voc_train, batch_size=BATCH_SIZE, shuffle=False)\n",
    "val_loader = DataLoader(voc_val, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(voc_test, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b19bf5ea-e1a5-4c45-883b-5fb57eb7de42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 448, 448]) torch.Size([64, 7, 7, 25])\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd349501-c0d8-4f17-b193-aa5e46958439",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0779b53-46a7-4e7a-9e72-e335c5a70cf0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = \"cpu\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165ccdf4-9c4d-4a3c-a070-70d8d38dc370",
   "metadata": {},
   "source": [
    "# Hyperparameters\n",
    "- S: dimensions of SxS grid\n",
    "- B: number of bounding boxes predicted per cell\n",
    "- C: number of classes\n",
    "- lambda_coord: penalty on incorrect localization loss\n",
    "- lambda_noobj: penalty on incorrect noobj confidence loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acc9e47e-6e57-46e5-8c03-b533c891426c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "S = 7\n",
    "B = 2\n",
    "C = 20\n",
    "lambda_coord = 5.0\n",
    "lambda_noobj = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bec5d95-0199-4552-95c7-4b6d97980fc4",
   "metadata": {},
   "source": [
    "# Training Setup\n",
    "Model, loss function, optimizer, scheduler and evaluation utils\n",
    "\n",
    "- Model: ResNet18 convolutional layers pretrained on ImageNet with 2 feedforward layers outputting a (N x S x S x (5B + C)) tensor\n",
    "- Loss: Squared Error Loss\n",
    "- Optimizer + Scheduler: Stochastic Gradient Descent with momentum of 0.9 and weight decay of 0.0005. We train with learning rate set to 1e-3 for the first 75 epochs, 1e-4 for the next 30 epochs, and 1e-5 for the final 30 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba24ea55-5d84-4847-9304-e39e5417c712",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "yolo = ResNet18YOLOv1(S=S, B=B, C=C).to(DEVICE)\n",
    "yolo_loss = YOLOv1Loss(S=S, B=B, C=C, lambda_coord=lambda_coord, lambda_noobj=lambda_noobj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d7944b6-02d3-4492-be2b-c9c9d4d5db41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 0.0005\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "optimizer = torch.optim.SGD(yolo.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = MultiStepLR(optimizer,\n",
    "                        milestones=[75, 105],\n",
    "                        gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55178899-b207-43a0-b351-d8508b46a2f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluating Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "762600af-25e1-4a7a-af61-a2914c892db0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_loss(model, criterion, dataloader):\n",
    "    total_loss = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, Y in dataloader:\n",
    "            X = X.to(DEVICE)\n",
    "            Y = Y.to(DEVICE)\n",
    "\n",
    "            pred = model(X)\n",
    "            loss = criterion(pred, Y)\n",
    "            total_loss += loss.item()\n",
    "            break\n",
    "            \n",
    "    N = len(dataloader)\n",
    "    # loss = total_loss / N\n",
    "    loss = total_loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fadd2fd-d7b5-467a-913d-84a7442e1e22",
   "metadata": {},
   "source": [
    "## Evaluating mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50f98021-62a9-4213-b921-ab77362527ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CONFIDENCE_THRESHOLD = 0.5\n",
    "IOU_THRESHOLD = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a01196e-993e-40a4-97e9-04d9c471aca0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bboxes(model, dataloader):\n",
    "    model.eval()\n",
    "    pred_bboxes = []\n",
    "    target_bboxes = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, Y in dataloader:\n",
    "            X, Y = X.to(DEVICE), Y.to(DEVICE)\n",
    "            pred = model(X)\n",
    "            \n",
    "            for i in range(len(X)):\n",
    "                x = pred[i]\n",
    "                y = Y[i]\n",
    "                \n",
    "                pred_bbox = get_bboxes(x, confidence_threshold=CONFIDENCE_THRESHOLD, iou_threshold=IOU_THRESHOLD, S=S, B=B, C=C)\n",
    "                target_bbox = get_bboxes(y, confidence_threshold=CONFIDENCE_THRESHOLD, iou_threshold=IOU_THRESHOLD, S=S, B=1, C=C)\n",
    "                \n",
    "                pred_bboxes.append(pred_bbox)\n",
    "                target_bboxes.append(target_bbox)\n",
    "    \n",
    "    return pred_bboxes, target_bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc8fec1a-683f-45b7-82d9-cb0aac3ffedb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_mAP(model, dataloader):\n",
    "    pred_bboxes, target_bboxes = bboxes(model, dataloader)\n",
    "    mAP = mean_average_precision(pred_bboxes, target_bboxes, iou_threshold=IOU_THRESHOLD, C=C)\n",
    "    \n",
    "    return mAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b524fa7b-cbca-43af-8efd-c6334c553433",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5430c9cb-2383-4030-b7c5-3e561e51aa3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1dc6eb43-0652-4fc0-8d22-f7720c24bc7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EPOCHS = 40\n",
    "\n",
    "def train(model, criterion, train_loader, val_loader, optimizer, scheduler):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_mAPs = []\n",
    "    \n",
    "    best_mAP = -float(\"inf\")\n",
    "    \n",
    "    N = len(train_loader)\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        # set to train mode\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        pbar = tqdm(train_loader, leave=False, desc=f\"Epoch [{epoch+1}/{EPOCHS}]: lr={lr}\")\n",
    "        \n",
    "        for i, (X, Y) in enumerate(pbar):\n",
    "            X, Y = X.to(DEVICE), Y.to(DEVICE)\n",
    "            pred = model(X)\n",
    "            loss = criterion(pred, Y)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # update progress bar\n",
    "            pbar.set_postfix(batch_loss=loss.item())\n",
    "        \n",
    "        # update learning rate with scheduler\n",
    "        scheduler.step()  \n",
    "        \n",
    "        # calculate metrics\n",
    "        train_loss = total_loss / N\n",
    "        # train_loss = total_loss\n",
    "        val_loss = compute_loss(model, criterion, val_loader)\n",
    "        # val_mAP = compute_mAP(model, val_loader)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        # val_mAPs.append(val_mAP)\n",
    "        \n",
    "#         if val_mAP > best_mAP:\n",
    "#             best_mAP = val_mAP\n",
    "            \n",
    "#             # save best model\n",
    "#             print(\"=> saving best model\")\n",
    "#             torch.save(model.state_dict(), \"best_model.pth\")\n",
    "        \n",
    "        # print(f\"Epoch [{epoch+1}/{EPOCHS}]: Loss={train_loss}, Val Loss={val_loss}, mAP={val_mAP}\")\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}]: Loss={train_loss}, Val Loss={val_loss}\")\n",
    "    \n",
    "    return {\n",
    "        \"train_losses\": train_losses,\n",
    "        \"val_losses\": val_losses,\n",
    "        \"val_mAP\": val_mAP,\n",
    "        \"epochs\": EPOCHS\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822011ce-19f5-4497-9efb-bc465004e0fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/40]: Loss=3.1734261333942415, Val Loss=5.162011623382568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/40]: Loss=3.1328529953956603, Val Loss=5.118073463439941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/40]: Loss=3.0633435606956483, Val Loss=5.193297386169434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/40]: lr=0.0001:  55%|█████▌    | 22/40 [00:35<00:29,  1.61s/it, batch_loss=2.58]"
     ]
    }
   ],
   "source": [
    "train_result = train(yolo, \n",
    "      yolo_loss, \n",
    "      train_loader=train_loader, \n",
    "      val_loader=val_loader, \n",
    "      optimizer=optimizer, \n",
    "      scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd14f5cd-d313-4de5-9035-22fead0a7930",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b841a7-200d-464d-a6d5-23178acf7aca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning:Python",
   "language": "python",
   "name": "conda-env-deeplearning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
